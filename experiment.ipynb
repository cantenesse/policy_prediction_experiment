{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning Experiment: ACORD Form to Policy JSON\n",
    "\n",
    "This notebook demonstrates using Claude to predict insurance policy attributes from ACORD forms using few-shot learning.\n",
    "\n",
    "## Experiment Overview\n",
    "- **Training Data**: 5 ACORD forms with corresponding policy JSON representations\n",
    "- **Test Data**: 1 ACORD form (the 6th one)\n",
    "- **Method**: Few-shot prompting with Claude Sonnet 4.5\n",
    "- **Evaluation**: Compare generated JSON against ground truth policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check API Key\n",
    "\n",
    "Make sure your Anthropic API key is set in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ANTHROPIC_API_KEY not found in environment.\nPlease set it by:\n1. Creating a .env file with: ANTHROPIC_API_KEY=your-key-here\n2. Or setting it in your shell: export ANTHROPIC_API_KEY='your-key-here'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m api_key = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mANTHROPIC_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mANTHROPIC_API_KEY not found in environment.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease set it by:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m1. Creating a .env file with: ANTHROPIC_API_KEY=your-key-here\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2. Or setting it in your shell: export ANTHROPIC_API_KEY=\u001b[39m\u001b[33m'\u001b[39m\u001b[33myour-key-here\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ API key found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: ANTHROPIC_API_KEY not found in environment.\nPlease set it by:\n1. Creating a .env file with: ANTHROPIC_API_KEY=your-key-here\n2. Or setting it in your shell: export ANTHROPIC_API_KEY='your-key-here'"
     ]
    }
   ],
   "source": [
    "api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"ANTHROPIC_API_KEY not found in environment.\\n\"\n",
    "        \"Please set it by:\\n\"\n",
    "        \"1. Creating a .env file with: ANTHROPIC_API_KEY=your-key-here\\n\"\n",
    "        \"2. Or setting it in your shell: export ANTHROPIC_API_KEY='your-key-here'\"\n",
    "    )\n",
    "print(\"✓ API key found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training Examples\n",
    "\n",
    "Load the first 5 ACORD forms and their corresponding policy JSONs as training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 5 training examples\n",
      "\n",
      "================================================================================\n",
      "PREVIEW: First Training Example\n",
      "================================================================================\n",
      "\n",
      "ACORD Form (first 500 chars):\n",
      "ACORD 125 - COMMERCIAL INSURANCE APPLICATION\n",
      "============================================\n",
      "\n",
      "AGENCY: Summit Insurance Brokers\n",
      "AGENT: Jennifer Martinez\n",
      "DATE: 03/15/2024\n",
      "APPLICATION NO: APP-2024-001\n",
      "\n",
      "APPLICANT INFORMATION\n",
      "---------------------\n",
      "Business Name: Green Valley Landscaping LLC\n",
      "DBA: Green Valley Lawn Care\n",
      "Mailing Address: 1245 Oak Street, Suite 200\n",
      "City: Portland, State: OR, ZIP: 97201\n",
      "Physical Address: Same as mailing\n",
      "Business Phone: (503) 555-0142\n",
      "Email: info@greenvalleylandscaping.com\n",
      "We\n",
      "\n",
      "...\n",
      "\n",
      "Policy JSON (first 500 chars):\n",
      "{\n",
      "  \"policy_number\": \"CMP-2024-789456\",\n",
      "  \"effective_date\": \"04/01/2024\",\n",
      "  \"expiration_date\": \"04/01/2025\",\n",
      "  \"insured\": {\n",
      "    \"business_name\": \"Green Valley Landscaping LLC\",\n",
      "    \"dba\": \"Green Valley Lawn Care\",\n",
      "    \"address\": \"1245 Oak Street, Suite 200, Portland, OR 97201\",\n",
      "    \"federal_id\": \"93-1234567\",\n",
      "    \"industry_code\": \"561730\",\n",
      "    \"industry_description\": \"Landscaping Services\"\n",
      "  },\n",
      "  \"policy_type\": \"Business Owners Policy (BOP)\",\n",
      "  \"premium\": {\n",
      "    \"total_annual_premium\": 18750,\n",
      "   \n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def load_training_examples():\n",
    "    \"\"\"Load the first 5 ACORD forms and their corresponding policies as training examples.\"\"\"\n",
    "    training_data = []\n",
    "    base_path = Path('.')\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        acord_path = base_path / f'acord_form_{i}.txt'\n",
    "        policy_path = base_path / f'policy_{i}.json'\n",
    "        \n",
    "        with open(acord_path, 'r') as f:\n",
    "            acord_form = f.read()\n",
    "        with open(policy_path, 'r') as f:\n",
    "            policy_json = f.read()\n",
    "            \n",
    "        training_data.append({\n",
    "            'acord_form': acord_form,\n",
    "            'policy_json': policy_json\n",
    "        })\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "training_data = load_training_examples()\n",
    "print(f\"✓ Loaded {len(training_data)} training examples\")\n",
    "\n",
    "# Show a preview of the first training example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREVIEW: First Training Example\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nACORD Form (first 500 chars):\")\n",
    "print(training_data[0]['acord_form'][:500])\n",
    "print(\"\\n...\\n\")\n",
    "print(\"Policy JSON (first 500 chars):\")\n",
    "print(training_data[0]['policy_json'][:500])\n",
    "print(\"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Test Case\n",
    "\n",
    "Load the 6th ACORD form (test case) and its ground truth policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test case loaded\n",
      "\n",
      "Test ACORD form length: 3166 characters\n",
      "Ground truth policy has 10 top-level fields\n"
     ]
    }
   ],
   "source": [
    "def load_test_case():\n",
    "    \"\"\"Load the 6th ACORD form (test case) and ground truth policy.\"\"\"\n",
    "    base_path = Path('.')\n",
    "    \n",
    "    with open(base_path / 'acord_form_6.txt', 'r') as f:\n",
    "        test_acord = f.read()\n",
    "    with open(base_path / 'policy_6.json', 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    return test_acord, ground_truth\n",
    "\n",
    "test_acord, ground_truth = load_test_case()\n",
    "print(\"✓ Test case loaded\")\n",
    "print(f\"\\nTest ACORD form length: {len(test_acord)} characters\")\n",
    "print(f\"Ground truth policy has {len(ground_truth)} top-level fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Few-Shot Prompt\n",
    "\n",
    "Construct a prompt that includes all training examples followed by the test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt created\n",
      "\n",
      "Prompt length: 35,698 characters\n",
      "Estimated tokens: ~8,924\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(training_data, test_acord):\n",
    "    \"\"\"Create the few-shot prompt for the LLM.\"\"\"\n",
    "    prompt = \"\"\"You are an insurance underwriting expert. Your task is to generate a JSON policy representation based on an ACORD insurance application form.\n",
    "\n",
    "I will show you 5 examples of ACORD forms and their corresponding policy JSON representations. Then, you will generate a policy JSON for a new ACORD form.\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add training examples\n",
    "    for i, example in enumerate(training_data, 1):\n",
    "        prompt += f\"=== EXAMPLE {i} ===\\n\\n\"\n",
    "        prompt += f\"ACORD FORM:\\n{example['acord_form']}\\n\\n\"\n",
    "        prompt += f\"CORRESPONDING POLICY JSON:\\n{example['policy_json']}\\n\\n\"\n",
    "        prompt += \"=\" * 80 + \"\\n\\n\"\n",
    "    \n",
    "    # Add test case\n",
    "    prompt += \"\"\"Now, based on the patterns you've observed in the examples above, generate a policy JSON for this new ACORD form:\n",
    "\n",
    "TEST ACORD FORM:\n",
    "\"\"\"\n",
    "    prompt += test_acord\n",
    "    prompt += \"\\n\\nGenerate the policy JSON following the same structure and logic as the examples. Return ONLY the JSON, no additional text.\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "prompt = create_prompt(training_data, test_acord)\n",
    "print(f\"✓ Prompt created\")\n",
    "print(f\"\\nPrompt length: {len(prompt):,} characters\")\n",
    "print(f\"Estimated tokens: ~{len(prompt) // 4:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Policy with Claude\n",
    "\n",
    "Call the Claude API to generate the policy JSON for the test ACORD form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_with_claude(prompt, api_key):\n",
    "    \"\"\"Use Claude to generate the policy JSON.\"\"\"\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    print(\"Calling Claude API...\")\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text\n",
    "\n",
    "try:\n",
    "    generated_text = generate_policy_with_claude(prompt, api_key)\n",
    "    \n",
    "    # Extract JSON from response (in case there's extra text)\n",
    "    start_idx = generated_text.find('{')\n",
    "    end_idx = generated_text.rfind('}') + 1\n",
    "    json_text = generated_text[start_idx:end_idx]\n",
    "    \n",
    "    generated_policy = json.loads(json_text)\n",
    "    print(\"✓ Policy JSON generated successfully\")\n",
    "    \n",
    "    # Save generated policy\n",
    "    with open('generated_policy_6.json', 'w') as f:\n",
    "        json.dump(generated_policy, f, indent=2)\n",
    "    print(\"✓ Generated policy saved to: generated_policy_6.json\")\n",
    "    \n",
    "    # Display preview\n",
    "    print(\"\\nGenerated Policy Preview:\")\n",
    "    print(json.dumps(generated_policy, indent=2)[:500] + \"\\n...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error generating policy: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare Generated Policy with Ground Truth\n",
    "\n",
    "Analyze the differences between the generated policy and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    \"\"\"Flatten nested dictionary for easier comparison.\"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            items.append((new_key, str(v)))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def compare_policies(generated, ground_truth):\n",
    "    \"\"\"Compare generated policy with ground truth and calculate metrics.\"\"\"\n",
    "    gen_flat = flatten_dict(generated)\n",
    "    truth_flat = flatten_dict(ground_truth)\n",
    "    \n",
    "    # Find common keys\n",
    "    common_keys = set(gen_flat.keys()) & set(truth_flat.keys())\n",
    "    gen_only_keys = set(gen_flat.keys()) - set(truth_flat.keys())\n",
    "    truth_only_keys = set(truth_flat.keys()) - set(gen_flat.keys())\n",
    "    \n",
    "    # Calculate matches\n",
    "    exact_matches = 0\n",
    "    close_matches = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    comparison_details = {\n",
    "        'exact_matches': [],\n",
    "        'close_matches': [],\n",
    "        'mismatches': [],\n",
    "        'missing_in_generated': [],\n",
    "        'extra_in_generated': []\n",
    "    }\n",
    "    \n",
    "    for key in common_keys:\n",
    "        gen_val = gen_flat[key]\n",
    "        truth_val = truth_flat[key]\n",
    "        \n",
    "        if gen_val == truth_val:\n",
    "            exact_matches += 1\n",
    "            comparison_details['exact_matches'].append({\n",
    "                'field': key,\n",
    "                'value': truth_val\n",
    "            })\n",
    "        elif isinstance(gen_val, (int, float)) and isinstance(truth_val, (int, float)):\n",
    "            # Check if numeric values are close (within 10%)\n",
    "            if truth_val != 0 and abs(gen_val - truth_val) / abs(truth_val) <= 0.10:\n",
    "                close_matches += 1\n",
    "                comparison_details['close_matches'].append({\n",
    "                    'field': key,\n",
    "                    'generated': gen_val,\n",
    "                    'ground_truth': truth_val\n",
    "                })\n",
    "            else:\n",
    "                mismatches += 1\n",
    "                comparison_details['mismatches'].append({\n",
    "                    'field': key,\n",
    "                    'generated': gen_val,\n",
    "                    'ground_truth': truth_val\n",
    "                })\n",
    "        else:\n",
    "            mismatches += 1\n",
    "            comparison_details['mismatches'].append({\n",
    "                'field': key,\n",
    "                'generated': gen_val,\n",
    "                'ground_truth': truth_val\n",
    "            })\n",
    "    \n",
    "    # Track missing and extra fields\n",
    "    for key in truth_only_keys:\n",
    "        comparison_details['missing_in_generated'].append({\n",
    "            'field': key,\n",
    "            'ground_truth': truth_flat[key]\n",
    "        })\n",
    "    \n",
    "    for key in gen_only_keys:\n",
    "        comparison_details['extra_in_generated'].append({\n",
    "            'field': key,\n",
    "            'generated': gen_flat[key]\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_fields = len(truth_flat)\n",
    "    accuracy = (exact_matches + close_matches) / total_fields if total_fields > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'total_fields_in_ground_truth': total_fields,\n",
    "        'total_fields_in_generated': len(gen_flat),\n",
    "        'exact_matches': exact_matches,\n",
    "        'close_matches': close_matches,\n",
    "        'mismatches': mismatches,\n",
    "        'missing_fields': len(truth_only_keys),\n",
    "        'extra_fields': len(gen_only_keys),\n",
    "        'accuracy': accuracy,\n",
    "        'exact_match_rate': exact_matches / total_fields if total_fields > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return metrics, comparison_details\n",
    "\n",
    "metrics, details = compare_policies(generated_policy, ground_truth)\n",
    "\n",
    "# Save detailed comparison\n",
    "with open('comparison_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'metrics': metrics,\n",
    "        'details': details\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"✓ Comparison complete\")\n",
    "print(\"✓ Detailed comparison saved to: comparison_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Display the key metrics from our comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Overall Accuracy: {metrics['accuracy']:.2%}\")\n",
    "print(f\"Exact Match Rate: {metrics['exact_match_rate']:.2%}\")\n",
    "print()\n",
    "print(f\"Total Fields (Ground Truth): {metrics['total_fields_in_ground_truth']}\")\n",
    "print(f\"Total Fields (Generated): {metrics['total_fields_in_generated']}\")\n",
    "print()\n",
    "print(f\"Exact Matches: {metrics['exact_matches']}\")\n",
    "print(f\"Close Matches: {metrics['close_matches']}\")\n",
    "print(f\"Mismatches: {metrics['mismatches']}\")\n",
    "print(f\"Missing Fields: {metrics['missing_fields']}\")\n",
    "print(f\"Extra Fields: {metrics['extra_fields']}\")\n",
    "print()\n",
    "\n",
    "# Show sample mismatches\n",
    "if details['mismatches']:\n",
    "    print(\"Sample Mismatches (first 5):\")\n",
    "    for mismatch in details['mismatches'][:5]:\n",
    "        print(f\"  • {mismatch['field']}\")\n",
    "        print(f\"    Generated: {mismatch['generated']}\")\n",
    "        print(f\"    Expected:  {mismatch['ground_truth']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Results Breakdown\n",
    "\n",
    "Create visualizations to better understand the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for plotting\n",
    "results_data = {\n",
    "    'Category': ['Exact Matches', 'Close Matches', 'Mismatches', 'Missing', 'Extra'],\n",
    "    'Count': [\n",
    "        metrics['exact_matches'],\n",
    "        metrics['close_matches'],\n",
    "        metrics['mismatches'],\n",
    "        metrics['missing_fields'],\n",
    "        metrics['extra_fields']\n",
    "    ]\n",
    "}\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "# Create two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['#2ecc71', '#27ae60', '#e74c3c', '#e67e22', '#95a5a6']\n",
    "bars = ax1.bar(df_results['Category'], df_results['Count'], color=colors)\n",
    "ax1.set_xlabel('Category', fontsize=12)\n",
    "ax1.set_ylabel('Number of Fields', fontsize=12)\n",
    "ax1.set_title('Field Comparison Breakdown', fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart for accuracy\n",
    "accuracy_data = {\n",
    "    'Correct\\n(Exact + Close)': metrics['exact_matches'] + metrics['close_matches'],\n",
    "    'Incorrect': metrics['mismatches'] + metrics['missing_fields']\n",
    "}\n",
    "colors_pie = ['#2ecc71', '#e74c3c']\n",
    "ax2.pie(accuracy_data.values(), labels=accuracy_data.keys(), autopct='%1.1f%%',\n",
    "       colors=colors_pie, startangle=90, textprops={'fontsize': 12})\n",
    "ax2.set_title(f'Overall Accuracy: {metrics[\"accuracy\"]:.1%}', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved to: experiment_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Detailed Mismatch Examination\n",
    "\n",
    "Let's examine the mismatches more closely to understand where the model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for detailed analysis\n",
    "if details['mismatches']:\n",
    "    df_mismatches = pd.DataFrame(details['mismatches'])\n",
    "    print(\"Top 10 Mismatches:\")\n",
    "    print(df_mismatches.head(10).to_string(index=False))\n",
    "    print()\n",
    "else:\n",
    "    print(\"No mismatches found!\")\n",
    "\n",
    "if details['missing_in_generated']:\n",
    "    df_missing = pd.DataFrame(details['missing_in_generated'])\n",
    "    print(\"\\nMissing Fields (not generated by model):\")\n",
    "    print(df_missing.head(10).to_string(index=False))\n",
    "    print()\n",
    "else:\n",
    "    print(\"\\nNo missing fields!\")\n",
    "\n",
    "if details['extra_in_generated']:\n",
    "    df_extra = pd.DataFrame(details['extra_in_generated'])\n",
    "    print(\"\\nExtra Fields (generated but not in ground truth):\")\n",
    "    print(df_extra.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo extra fields!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment demonstrates the effectiveness of few-shot learning for insurance policy prediction. The results can help inform:\n",
    "\n",
    "1. **Model Performance**: How well does the LLM understand insurance underwriting patterns?\n",
    "2. **Field Accuracy**: Which policy fields are predicted most/least accurately?\n",
    "3. **Business Viability**: Is this approach suitable for production use?\n",
    "4. **Improvement Opportunities**: Where should we focus optimization efforts?\n",
    "\n",
    "### Next Steps\n",
    "- Analyze which types of fields (premiums, coverages, limits) are most accurate\n",
    "- Test with different numbers of training examples\n",
    "- Try chain-of-thought prompting for better reasoning\n",
    "- Compare different LLM models\n",
    "- Implement validation rules for generated policies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
